{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Attention\n",
    "There is 3 main ways to do attention: encoder-decoder attention, causal attention and bi-directional self-attention. I already implemented the first in this [repo](https://github.com/abtraore/nmt-rnn-pytorch-from-scratch). I will then focus on the later 2. In the heart of any attention scheme used in a transformer reside a **dot product attention**, let focus on that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Dot product attention\n",
    "\n",
    "The dot product attention can be computed using the formula: $$\\textrm{softmax} \\left(\\frac{Q K^T}{\\sqrt{d}} + M \\right) V$$ Where the (optional, but default) scaling factor $\\sqrt{d}$ is the square root of the embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy q,k,v matrix. They all should have the same shape.\n",
    "q = torch.tensor([[1,0,0],[0,1,0]],dtype=torch.float)\n",
    "k = torch.tensor([[1,2,3],[4,5,6]],dtype=torch.float)\n",
    "v = torch.tensor([[0,1,0],[1,0,1]],dtype=torch.float)\n",
    "\n",
    "# The mask used if we want to compute the causal sot product attention.\n",
    "m = torch.tensor([[1,0],[1,0]],dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:\n",
      "tensor([[1., 0.],\n",
      "        [1., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dot_product_attention(q,k,v,mask=None,scale=True):\n",
    "\n",
    "    matmul_qk = q @ k.T\n",
    "\n",
    "    if scale:\n",
    "        dk = torch.tensor(k.shape[-1],dtype=torch.float)\n",
    "        matmul_qk /= torch.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        matmul_qk = matmul_qk + (1-mask) * -1e9\n",
    "\n",
    "    attention_weights = F.softmax(matmul_qk,dim=-1)\n",
    "\n",
    "    print(f\"Attention weights:\\n{attention_weights}\")\n",
    "\n",
    "    attention_output = attention_weights @ v\n",
    "\n",
    "    return attention_output\n",
    "\n",
    "dot_product_attention(q,k,v,mask=m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Causal dot product attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal mask:\n",
      "tensor([[1., 0.],\n",
      "        [1., 1.]])\n",
      "\n",
      "Attention weights:\n",
      "tensor([[1.0000, 0.0000],\n",
      "        [0.1503, 0.8497]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.0000, 0.0000],\n",
       "        [0.8497, 0.1503, 0.8497]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def causal_dot_product_attention(q,k,v,scale=True):\n",
    "    \n",
    "    # The size of the mask equal the penultimate dimention of the query. (seq length)\n",
    "    mask_size = q.shape[-2]\n",
    "\n",
    "    # Creates a matrix with ones below the diagonal and 0s above.\n",
    "    mask = torch.tril(torch.ones(mask_size,mask_size))\n",
    "    \n",
    "    print(f\"Causal mask:\\n{mask}\\n\")\n",
    "\n",
    "    return dot_product_attention(q,k,v,mask=mask,scale=scale)\n",
    "\n",
    "causal_dot_product_attention(q,k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Masking\n",
    "Masking is a key component in building transformers. There is two type of mask : the ***padding mask*** and the ***look-ahead*** mask. Both contribute to appropriatly compute the softmax by appliying the proper weights to words.\n",
    "\n",
    "# 2.1 - Padding Mask\n",
    "When fedding an input batch to a model, sentences can have different length, so it is important to pad them using 0s. Longer sentences that are supperior to the maximum length will be truncaded. Note that to compute the attention we will proceded as we did in 1.1. The 0s will be put to -inf(a small negative number) so that they won't affect the softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_padding_mask(token_ids):\n",
    "    # All the padding will have 0 as value\n",
    "    mask = 1 - (token_ids == 0).float()\n",
    "    #Add an extra dimension to allow broadcasting.\n",
    "    mask = mask.unsqueeze(1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No padding mask softmax: \n",
      "tensor([[[7.2960e-01, 2.6840e-01, 6.6531e-04, 6.6531e-04, 6.6531e-04]],\n",
      "\n",
      "        [[8.4437e-02, 2.2952e-01, 6.2391e-01, 3.1063e-02, 3.1063e-02]],\n",
      "\n",
      "        [[8.3393e-01, 4.1519e-02, 4.1519e-02, 4.1519e-02, 4.1519e-02]]])\n",
      "\n",
      "Padding mask softmax: \n",
      "tensor([[[0.7311, 0.2689, 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0900, 0.2447, 0.6652, 0.0000, 0.0000]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[7., 6., 0., 0., 0.], [1., 2., 3., 0., 0.], [3., 0., 0., 0., 0.]])\n",
    "mask = create_padding_mask(x)\n",
    "x_extented = x.unsqueeze(1)\n",
    "print(f\"No padding mask softmax: \\n{F.softmax(x_extented,dim=-1)}\")\n",
    "\n",
    "print(f\"\\nPadding mask softmax: \\n{F.softmax(x_extented + (1 - mask) * -1e9,dim=-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 - Look-ahead Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 1.]]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_look_ahead_mask(seq_length):\n",
    "    mask = torch.tril(torch.ones((1,seq_length,seq_length)))\n",
    "    return mask\n",
    "\n",
    "x = torch.randn(1,3)\n",
    "x = create_look_ahead_mask(x.shape[1])\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
